{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e55383e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from datasets import load_dataset   # using huggingface datasets\n",
    "\n",
    "from tqdm import tqdm    # loading bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef8c62d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num of workers for .map() call\n",
    "nproc = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98b5bdeb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13248/457283554.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# loading the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"openwebtext\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[1;32m   1780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m     \u001b[0;31m# Download and prepare data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1782\u001b[0;31m     builder_instance.download_and_prepare(\n\u001b[0m\u001b[1;32m   1783\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m         \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m         \u001b[0;31m# File locking only with local paths; no file locking on GCS or S3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mFileLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_local\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnullcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    794\u001b[0m             \u001b[0;31m# Check if the data already exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m             \u001b[0mpath_join\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_local\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mposixpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/utils/filelock.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/utils/filelock.py\u001b[0m in \u001b[0;36macquire\u001b[0;34m(self, timeout, poll_intervall)\u001b[0m\n\u001b[1;32m    280\u001b[0m                         \u001b[0;34mf\"Lock {lock_id} not acquired on {lock_filename}, waiting {poll_intervall} seconds ...\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                     )\n\u001b[0;32m--> 282\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll_intervall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;31m# Something did go wrong, so decrement the counter.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loading the dataset\n",
    "dataset = load_dataset(\"openwebtext\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef205898",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- the dataset occupies ~52 GB in huggingface\n",
    "- about 8M documents\n",
    "- by default : only contains the train split\n",
    "- \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c0d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating train - val split\n",
    "split_dataset = dataset['train'].train_test_split(test_size=0.005, seed=44, shuffle = True)\n",
    "\n",
    "\n",
    "# split_dataset['val'] = split_dataset.pop('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e23a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the encoding function for tokenization\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "def tokenizer(data):\n",
    "    ids = enc.encode_ordinary(data['text'])\n",
    "    # adding < end of text > token \n",
    "    ids.append(enc.eot_token)\n",
    "    out = {'ids' : ids, 'len': len(ids)}\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d938943b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8b20ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the dataset\n",
    "tokenized = split_dataset.map(tokenizer, remove_columns=['text'],\n",
    "                              desc = \"Tokenizing the splits\",\n",
    "                              num_proc = nproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d15115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating all the ids in 'train' and 'val' dataset into\n",
    "# one larger file '.bin' to be used later\n",
    "\n",
    "for split, dset in tokenized.items():\n",
    "    arr_len = np.sum(dset['len'])\n",
    "    filename = os.path.join(os.path.dirname(__file__), f'{split}.bin')\n",
    "    dtype = np.uint16\n",
    "    arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "    \n",
    "    print(f\"Writing {filename} .....\")\n",
    "    \n",
    "    idx = 0\n",
    "    \n",
    "    for data in tqdm(dset):\n",
    "        arr[idx : idx + data['len']] = data['ids']\n",
    "        idx += data['len']\n",
    "    \n",
    "    arr.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
